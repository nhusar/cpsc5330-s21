{"cells":[{"cell_type":"markdown","source":["#### Streaming Log Processing\n","\n","This is a simple exercise in log processing.  The log files come from various servers at various time points.\n","Each record in a log file is of the form\n","\n","```serverID,severity,timestamp```\n","\n","* serverID is a string unique to the server\n","* severity is a value 2 (no error, just a service call), or 1 (minor error), or 0 (fatal/severe error)\n","* timestamp is an integer starting at 1 (bigger numbers mean later)\n","\n","For this exercise these log files will be \"delivered\" by being placed in a directory, for example ```/FileStore/tables/logdata-live```.\n","The log files for this small example have two servers, s1 and s2 and log records for times 1 through 10.\n","The files are delivered with one file per server for five time units.  For example, the file s115.csv has records for server 1 for times 1 through 5.\n","\n","You want to process these new records incrementally, and are interested in these two \"reports\"\n","\n","*  The *volume report* reports by server the number of SEV2 events divided by the number of time units.  The number of time units for our purposes is (max(timestamp) - min(timestamp)) + 1.  This volume report will not be cumulative -- i.e. every time new log data comes in, the mapping from server to sev2 volume is updated\n","* The *sev0 log* -- this is a sequence of records of the form ```serverID timestamp``` recording a SEV0 event reported by the server.  This report grows over time -- each time a new log file is processed, new records are appended to the end.\n","\n","Your final result should be two streaming queries\n","* One that *modifies* the volume report, which is stored in memory\n","* One that *appends to* the sev0 log, which is stored as a Parquet file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cad7123-c888-48a6-81ae-91543e02d59c"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, LongType\n","from pyspark.sql.functions import col, min, max, count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0b85e3-2fe4-4cec-8f56-2a9be19d3b56"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6265f05e-1ddb-4d6a-a5bf-245f6e0af785"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["LOG_DATA_PATH_SOURCE = \"/FileStore/tables/LogData-source/\"\n","\n","LOG_DATA_PATH = \"/FileStore/tables/LogData/\"\n","\n","SEV0_OUTPUT = \"/FileStore/tables/LogData-output/\"\n","SEV0_CHECKPOINTS = \"/FileStore/tables/LogData-checkpoints/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5922fb2d-de2f-4c19-bbe5-1c8849d2794e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Verify that there are four log files in your staging directory\n","# If using EMR notebook, do this at the AWS console and leave this cell blank\n","dbutils.fs.ls(LOG_DATA_PATH_SOURCE)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e81f4bd-6f64-4cb9-9a04-b4356713792e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: [FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s115.csv&#39;, name=&#39;s115.csv&#39;, size=16000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s1610.csv&#39;, name=&#39;s1610.csv&#39;, size=820),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s215.csv&#39;, name=&#39;s215.csv&#39;, size=40000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s2610.csv&#39;, name=&#39;s2610.csv&#39;, size=8210)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s115.csv&#39;, name=&#39;s115.csv&#39;, size=16000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s1610.csv&#39;, name=&#39;s1610.csv&#39;, size=820),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s215.csv&#39;, name=&#39;s215.csv&#39;, size=40000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData-source/s2610.csv&#39;, name=&#39;s2610.csv&#39;, size=8210)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create the schema for the log files\n","logSchema = StructType([\n","    StructField(\"serverID\", StringType(), True),\n","    StructField(\"severity\", LongType(), True),\n","    StructField(\"timestamp\", LongType(), True)\n","])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34ff72b0-61d4-4e92-b478-c0c84dc9c375"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create the streaming DataFrame (readStream) on your log directory, using the schema you just created\n","logStream = spark.readStream \\\n","    .schema(logSchema) \\\n","    .option(\"maxFilesPerTrigger\", 1) \\\n","    .csv(LOG_DATA_PATH)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a513f133-14fd-4e46-bba8-855f401aadc0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use the data frame you just created to create another data frame with the \n","# sev2 volume report.  It should have columns 'serverID' and 'avgVolume'\n","volumeStream = logStream \\\n","    .filter(col(\"severity\") == 2) \\\n","    .groupBy(\"serverID\") \\\n","    .agg(min(\"timestamp\"), max(\"timestamp\"), count(\"timestamp\")) \\\n","    .withColumn(\"avgVolume\", col(\"count(timestamp)\") / (col(\"max(timestamp)\") - col(\"min(timestamp)\") + 1)) \\\n","    .select(col(\"serverID\"), col(\"avgVolume\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e4dd595-b686-4ba0-9baf-3326c63995f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create and start a query (writeStream) that generates the sev2 report;  it is an in-memory sink.\n","volumeQuery = volumeStream.writeStream \\\n","    .queryName(\"sev2_volume\") \\\n","    .format(\"memory\") \\\n","    .outputMode(\"complete\") \\\n","    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad420b2b-d748-4c5b-b568-5369c44c9396"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Clean up the data\n","dbutils.fs.rm(LOG_DATA_PATH, recurse=True)\n","dbutils.fs.mkdirs(LOG_DATA_PATH)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfa43796-b1b7-475c-a720-6f914e637c60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Write a (very simple) spark SQL query to show the contents of your query.  It should initially be empty\n","spark.sql(\"SELECT * FROM sev2_volume\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a44bb60-19c9-4f50-9278-641fb4806ca2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+---------+\n|serverID|avgVolume|\n+--------+---------+\n+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+\nserverID|avgVolume|\n+--------+---------+\n+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Copy two files into your 'live data' directory for both servers, for both servers and time period 1 through 5\n","# If using EMR notebook, do this at the AWS console and leave this cell blank\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s115.csv\", LOG_DATA_PATH + \"/\")\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s215.csv\", LOG_DATA_PATH + \"/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cd1418b-7ed3-4713-a5b7-7f8162fbfa2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Rerun the query to show that the sev2 volume report has been updated\n","spark.sql(\"SELECT * FROM sev2_volume\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f245ecc8-1a90-4a36-9c2d-c1d25b5a2827"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+---------+\n|serverID|avgVolume|\n+--------+---------+\n|      s2|    920.0|\n|      s1|    379.4|\n+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+\nserverID|avgVolume|\n+--------+---------+\n      s2|    920.0|\n      s1|    379.4|\n+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Now copy the log files for times 6 to 10\n","# If using EMR notebook, do this at the AWS console and leave this cell blank\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s1610.csv\", LOG_DATA_PATH + \"/\")\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s2610.csv\", LOG_DATA_PATH + \"/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0589690-9a47-49ac-b303-44e6fad7da49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Run the query again to verify that the report was updated. Be sure to wait for a little while\n","# to make sure the query is updated.\n","spark.sql(\"SELECT * FROM sev2_volume\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4246b60-8de0-4484-b6f3-274eb1ca3ad5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+---------+\n|serverID|avgVolume|\n+--------+---------+\n|      s2|    519.9|\n|      s1|    199.7|\n+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+\nserverID|avgVolume|\n+--------+---------+\n      s2|    519.9|\n      s1|    199.7|\n+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### The SEV0 log"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b8e2417-e879-45bf-a99e-bedb89fd9e81"}}},{"cell_type":"code","source":["# Delete all files from your \"live\" directory\n","# If using EMR notebook, do this at the AWS console and leave this cell blank\n","# Clean up the data\n","dbutils.fs.rm(LOG_DATA_PATH, recurse=True)\n","dbutils.fs.mkdirs(LOG_DATA_PATH)\n","\n","dbutils.fs.rm(SEV0_OUTPUT, recurse=True)\n","dbutils.fs.mkdirs(SEV0_OUTPUT)\n","\n","dbutils.fs.rm(SEV0_CHECKPOINTS, recurse=True)\n","dbutils.fs.mkdirs(SEV0_CHECKPOINTS)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35611c93-c48c-4b77-96d9-02e923c5585f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[17]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create a data frame on top of your original data frame that holds the raw data, \n","# this data frame for the sev0 report is just <serverID> <time stamp>\n","errorStream = logStream \\\n","    .filter(col(\"severity\") == 0) \\\n","    .select(col(\"serverID\"), col(\"timestamp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"744e269d-0f08-4634-aca5-bd9d4aac8741"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Create a query on your sev0 data frame that writes the table to a parquet file, \n","#  appending new records to the file\n","# https://stackoverflow.com/questions/55859868/pyspark-structured-streaming-write-to-parquet-in-batches\n","errorQuery = errorStream.writeStream \\\n","    .queryName(\"sev0_log\") \\\n","    .format(\"parquet\") \\\n","    .outputMode(\"append\") \\\n","    .option(\"path\", SEV0_OUTPUT) \\\n","    .option(\"checkpointLocation\", SEV0_CHECKPOINTS) \\\n","    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e9bb771-f5ba-4295-be8a-061363ab8eb5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Display the query content by reading the parquet file (it should be empty)\n","spark.read.parquet(SEV0_OUTPUT).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5766357-c0ca-434b-83e7-29824ee68976"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1563672191872842&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Display the query content by reading the parquet file (it should be empty)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>SEV0_OUTPUT<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, *paths, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    483</span>                        int96RebaseMode=int96RebaseMode)\n<span class=\"ansi-green-intense-fg ansi-bold\">    484</span> \n<span class=\"ansi-green-fg\">--&gt; 485</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    487</span>     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 116</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Unable to infer schema for Parquet at . It must be specified manually</div>","errorSummary":"<span class=\"ansi-red-fg\">AnalysisException</span>: Unable to infer schema for Parquet at . It must be specified manually","metadata":{},"type":"ipynbError","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1563672191872842&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># Display the query content by reading the parquet file (it should be empty)</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>SEV0_OUTPUT<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, *paths, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    483</span>                        int96RebaseMode=int96RebaseMode)\n<span class=\"ansi-green-intense-fg ansi-bold\">    484</span> \n<span class=\"ansi-green-fg\">--&gt; 485</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_df<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_spark<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> paths<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    487</span>     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 116</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Unable to infer schema for Parquet at . It must be specified manually</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Copy in the files for timestamp 1 through 5\n","# If using EMR notebook, do this at the AWS console and leave this cell blank\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s115.csv\", LOG_DATA_PATH + \"/\")\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s215.csv\", LOG_DATA_PATH + \"/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4aa8b252-8528-43ae-a8b6-0efa07c2e54b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Display the query again by reading the parquet file.  Are there new records?\n","spark.read.parquet(SEV0_OUTPUT).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"118d2643-5e50-4119-aa20-eb61d4c2ef5c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+---------+\n|serverID|timestamp|\n+--------+---------+\n|      s1|        5|\n|      s1|        5|\n|      s1|        5|\n+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+\nserverID|timestamp|\n+--------+---------+\n      s1|        5|\n      s1|        5|\n      s1|        5|\n+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Copy in the files for timestamp 6 through 10\n","# If using EMR notebook, do this at the AWS console and leave this cell blank\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s1610.csv\", LOG_DATA_PATH + \"/\")\n","dbutils.fs.cp(LOG_DATA_PATH_SOURCE + \"/s2610.csv\", LOG_DATA_PATH + \"/\")\n","dbutils.fs.ls(LOG_DATA_PATH)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b799364-f529-48d3-9557-56556433fde3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[33]: [FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s115.csv&#39;, name=&#39;s115.csv&#39;, size=16000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s1610.csv&#39;, name=&#39;s1610.csv&#39;, size=820),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s215.csv&#39;, name=&#39;s215.csv&#39;, size=40000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s2610.csv&#39;, name=&#39;s2610.csv&#39;, size=8210)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[33]: [FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s115.csv&#39;, name=&#39;s115.csv&#39;, size=16000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s1610.csv&#39;, name=&#39;s1610.csv&#39;, size=820),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s215.csv&#39;, name=&#39;s215.csv&#39;, size=40000),\n FileInfo(path=&#39;dbfs:/FileStore/tables/LogData/s2610.csv&#39;, name=&#39;s2610.csv&#39;, size=8210)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Display the query again by reading the parquet file.  Are there new records?\n","# dbutils.fs.ls(SEV0_OUTPUT)\n","spark.read.parquet(SEV0_OUTPUT).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7b834a6-3bde-4951-96a5-8499cb7c63a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+---------+\n|serverID|timestamp|\n+--------+---------+\n|      s1|        5|\n|      s1|        5|\n|      s1|        5|\n|      s2|        9|\n+--------+---------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+\nserverID|timestamp|\n+--------+---------+\n      s1|        5|\n      s1|        5|\n      s1|        5|\n      s2|        9|\n+--------+---------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Be tidy, stop all your streaming queries!\n","volumeQuery.stop()\n","errorQuery.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca26c769-6346-4116-9c53-d8d2b0ecb63a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Verify that there are no active streams\n","spark.streams.active"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"252d36a3-6ccb-45d1-9a15-ddd1a1afd279"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[38]: []</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: []</div>"]}}],"execution_count":0},{"cell_type":"code","source":[],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3d94df0-3854-4596-96d1-9b8c9d3a4ecf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"language_info":{"codemirror_mode":{"name":"python","version":3},"mimetype":"text/x-python","name":"pyspark","pygments_lexer":"python3"},"name":"StreamingLab","notebookId":3750749627479206,"kernelspec":{"display_name":"PySpark","language":"","name":"pysparkkernel"},"application/vnd.databricks.v1+notebook":{"notebookName":"Lab8","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1563672191872824}},"nbformat":4,"nbformat_minor":0}